---
title: "Pratical Machine Learning Project"
author: "João Pedro Schmitt"
date: "16 de novembro de 2016"
output: html_document
---

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

#### Required libraries

```{r message=FALSE, results='hide', warning=FALSE}
library(dplyr)
library(randomForest)
library(rpart)
library(rattle)
library(caret)
```

### Preparing the data

Preparation of the data for processment

#### Obtaining data

The training data for this project are available here:

[Training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:

[Testing data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

This project has the objective of simulated a model capable of predict possible outcomes based in collected data.

```{r}
if(!file.exists("training.csv"))
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv")

if(!file.exists("testing.csv"))
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv")

#We considered "NA" and "#DIV/0!" as NA values.
training <- read.csv("training.csv", na.strings = c("NA","#DIV/0!"))
testing <- read.csv("testing.csv", na.strings = c("NA","#DIV/0!"))
```

#### Data cleaning

The first thing was remove the useless characteristics and columns without values. Considered only variables that contains *belt*, *arm* or *dumbbell* as name for the predictors, and *classe* or *problem* as label for the outcome. After that, was processed variables by removing all columns that contain all values null.

```{r}
training <- training[,grepl("belt|arm|dumbbell|classe", names(training))]
testing <- testing[,grepl("belt|arm|dumbbell|problem", names(testing))]
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
```

Was created the partition using 75% as training and the remaining as testing.

```{r}
inTrain <- createDataPartition(y = training$classe, p = 3/4, list = FALSE)
trainingSet <- training[inTrain,]
testingSet <- training[-inTrain,]
```

### Predicting 

All models were made using a crossfold validation with 3 breaks in the training set, we use a low value to get more bias but less variance. The first time was started with some simulations to find the best predictors, after were used the most significative predictors to build the final model. Was used PCA to reduce the most significative predictors and was removed by hand the most significative predictors with correlation upper than 80%.

#### Classification trees

The first model was did using recursive partitioning tree (rpart), after was plotted the graph to show the decision tree.

```{r fig.align='center', cache=TRUE}
modRPART <- train(classe ~ ., 
                  method = "rpart", 
                  data = trainingSet, 
                  trControl = trainControl(method = "cv", number = 3))
fancyRpartPlot(modRPART$finalModel)
```

We observed that the simple decision tree has low confidence, we can prove this checking the confusion matrix.

```{r}
cmRPART <- confusionMatrix(predict(modRPART, testingSet), testingSet$classe)
print(cmRPART)
```

In Accuracy we had obtained around `r round(cmRPART$overall[1] * 100, 2)`%. A very low accuracy. The next step is use another algorithm of classification.

#### Random forest

Was used the Random Forest algorithm to predict the classe outcome with all other variables as predictors, after that, we predict the training set and plot a confusion matrix to show the accuracy of the model:

```{r cache=TRUE, fig.align='center'}
rf <- randomForest(classe ~ ., data = trainingSet)
cmRF <- confusionMatrix(predict(rf, testingSet), testingSet$classe)
print(cmRF)
```

Now we achieved `r round(cmRF$overall[1] * 100, 2)`%, very better than classification tree, but now we can be getting some overfitting because we are using all variables as predictors, lets try reduce some variables. To see the most relevant variables we plot the importance plot order descencending and we select the top 15 variables to remove the more correlated.

```{r}
relevantPrecitors <- names(rf$importance[order(rf$importance, decreasing = T),][1:10])
varImpPlot(rf)
```

Based in the plot, we select the most important predictors: **`r relevantPrecitors`**, and reduce the dataset. After selected the most important predictors we calculated the correlation between theirs to evict overfitting with a correlation around 80%.

```{r cache=TRUE}
# Reducing the dataset
redTrain <- trainingSet[,c("classe", relevantPrecitors)]
redTest <- testingSet[,c("classe", relevantPrecitors)]
# Checking the most correlated variables
varCorr <- abs(cor(redTrain[,-1]))
diag(varCorr) <- 0
correlated <- which(varCorr > 0.8, arr.ind = T)
print(correlated)
```

Based this was removed the index -2 **yaw_belt**, and was generated a new model with random forest using the selected variables. After that, was predicted the testing set and did a confusion matrix to get the accuracy.

```{r cache=TRUE}
relevantPrecitors <- relevantPrecitors[c(-2)]
redTrain <- trainingSet[,c("classe", relevantPrecitors)]
redTest <- testingSet[,c("classe", relevantPrecitors)]
# Generate a new model
modRFRed <- train(classe ~ ., 
                method = "rf", 
                data = redTrain,
                trControl = trainControl(method = "cv", number = 3))
cmRFRed <- confusionMatrix(predict(modRFRed, redTest), redTest$classe)
print(cmRFRed)
```

Reducing the most significative predictors, the result lead was `r round(cmRFRed$overall[1] * 100, 2)`% for a model very more simple with low lost of accuracy.

#### Using PCA

Was modeled a new model with PCA (Principal Component Analysis) to compare with the last model, was used PCA to limit the explain of variation to around 80% to fit a new model with random forest. After that, was predicted the testing set and produced a confusion matrix to gets the accuracy.

```{r cache=TRUE, message=FALSE, warning=FALSE}
 modRFPCA <- train(classe ~ ., 
                method = "rf", 
                data = trainingSet, 
                preProcess = "pca",
                trControl = trainControl(method = "cv", 
                                        number = 3, 
                                        preProcOptions = list(thresh = 0.8)))
cmRFPCA <- confusionMatrix(predict(modRFPCA, testingSet), testingSet$classe)
print(cmRFPCA)
```

Was lost some accuracy `r round(cmRFPCA$overall[1] * 100, 2)` but was simplified the model with less variables. With base in the generated models, was selected the model with manual selection of variables, due the accuracy to predict the testing challenge.

### Predicting the testing set

The test set was predicted with the model with the predictors reduced manually. With the selected model the out of error sample was `r round((1 - cmRFRed$overall[1]) * 100, 2)`%, indicating a good model based on the prediction.

```{r results="asis"}
# Predicted the results
result <- predict(modRFRed, testing)
# Generated a data frame
dataTable <- data.frame("Problem" = testing$problem_id, "Classe" = result)
# Create a table of the results
library(pander)
pandoc.table(dataTable)
```

And to finish the result was written in files named by the problem with the predited result.

```{r}
dir.create("result")
for(i in 1:length(result)){
    filename = paste0("result/problem_", testing$problem_id[i] ,".txt")
    write.table(result[i], 
                file=filename, 
                quote=FALSE, 
                row.names=FALSE, 
                col.names=FALSE)
}
```